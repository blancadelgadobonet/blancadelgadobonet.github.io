<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>localization | Hi, I'm Blanca</title> <meta name="author" content="Blanca Delgado"> <meta name="description" content="localizing a camera using beacons"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://blancadelgadobonet.github.io/projects/project_p3_localization/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Hi, I'm Blanca</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">localization</h1> <p class="post-description">localizing a camera using beacons</p> </header> <article> <p><strong>Robots aim to interact with their surroundings</strong>. To fulfil this objective, it is essential to know <strong>where the robot is within the scene</strong>. Localization is the process of determining where a mobile robot is with respect to its environment. Just like humans determine their position according to what they can see or hear, robots can use information from the scene to localize themselves. A key sensor to retrieve data from the robot’s surroundings is the camera, in fact, one way to obtain the position of a robot is by detecting known beacons in images captured from the scene with a calibrated camera. In this practice, <strong>visual autolocalization based on beacons</strong> will be implemented using Python.</p> <h2 id="geometric-autolocalization-using-beacons">Geometric autolocalization using beacons</h2> <p><strong>Beacons</strong> are patterns easy to distinguish and hard to confound. A popular design is the ArUco marker, a square of N-by-N black and white pixels, of known size. Autolocalization using beacons consists on estimating the position of the camera – from which the position of the robot can be computed – by detecting a known marker with said known camera. Knowing the camera implies knowing its intrinsic parameters, for which its <strong>calibration</strong> is necessary. Using the detected points of the maker in the image (and their corresponding points in the real world), the <strong>Perspective-n-Point (PnP)</strong> algorithm allows estimating the pose (i.e., position and orientation) of an object with respect to a known camera (or the pose of the camera with respect to the object). In sum, the process can be divided in three steps: <em>1) calibration</em> of the camera to obtain its intrinsic parameters, <em>2) detection of the marker</em> in the image, <em>3) computation of the pose</em> of the object (or camera) using the intrinsic parameters, the detected points of the marker in the image and the known points of the marker in the real world.</p> <p>The final result looks as follows:</p> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/A_zOSGOU7Y4" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%;margin-bottom:5%;margin-top:5%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/NuGvpLIXxqs" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%;margin-bottom:5%;margin-top:5%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>The camera is first calibrated with a chessboard pattern. Once calibrated, the marker is detected within the image and the pose is estimated and displayed. The position of the camera – its center – with respect to the reference point within the marker, is indicated by the red dot. The orientation of the camera, i.e., its x, y and z axes, are represented in red, green and blue, respectively. Note that the camera is always pointing towards the marker (we would not be able to localize it otherwise!).</p> <h3 id="okey-but-how">Okey, but how?</h3> <p>First of all, we printed (in a standard printer): 1) a <strong>calibration pattern</strong> (Figure 1, right), that can be found in <a href="https://github.com/opencv/opencv/blob/4.x/doc/pattern.png" rel="external nofollow noopener" target="_blank">OpenCV’s GitHub</a>, and 2) a <strong>marker</strong> (Figure 2, left), that can be automatically generated at <a href="https://chev.me/arucogen/" rel="external nofollow noopener" target="_blank">ArUco markers generator!</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/robotics/aruco-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/robotics/aruco-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/robotics/aruco-1400.webp"></source> <img src="/assets/img/robotics/aruco.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/robotics/pattern-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/robotics/pattern-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/robotics/pattern-1400.webp"></source> <img src="/assets/img/robotics/pattern.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Left: ArUco marker (4-by-4, 100 mm per side). Right: calibration pattern (9-by-6, 25 mm per side). </div> <p>Then, we created Python 3.10 Project (available in <a href="https://github.com/blancadelgadobonet/robotics.git" rel="external nofollow noopener" target="_blank">GitHub</a>) using a MacBook Pro 2021 (Apple M1 Pro), that was comprised of one main file (<code class="language-plaintext highlighter-rouge">main.py</code>), two sets of functions (<code class="language-plaintext highlighter-rouge">calibration.py</code> and <code class="language-plaintext highlighter-rouge">localization.py</code>), and one figure (<code class="language-plaintext highlighter-rouge">aruco.png</code>).</p> <p>The objective of the script was to <strong>acquire real-time images</strong> (either from the computer’s webcam or an iPhone plugged to the computer), <strong>calibrate</strong> the camera after capturing several images of a calibration pattern, <strong>localizing</strong> a marker (with a specific ID, set by the user) when present in said images and <strong>displaying</strong> the position of the camera with respect to the reference point of the marker.</p> <p>To <strong>calibrate</strong> a camera, images of a known pattern needed to be taken: detected 2D points of the image (in pixels) can be compared to known 3D points of the scene (in, e.g., millimeters), and the relation between 2D and 3D points can be extracted. To that means, the printed calibration pattern was captured by the camera (N=100 times, although less images can work); the bidimensional points of interest from the pattern were detected (using <code class="language-plaintext highlighter-rouge">cv2.findChessboardCorners</code>), displayed (using <code class="language-plaintext highlighter-rouge">cv2.drawChessboardCorners</code>, Figure 2), and matched to their corresponding three-dimensional points (generated using a custom function <code class="language-plaintext highlighter-rouge">tools.calibration.get_chessboard_points</code>); and the relation between all sets of captured 2D points and their known 3D points was approximated (using <code class="language-plaintext highlighter-rouge">cv2.calibrateCamera</code>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/robotics/eg-calibration-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/robotics/eg-calibration-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/robotics/eg-calibration-1400.webp"></source> <img src="/assets/img/robotics/eg-calibration.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Calibration pattern with detected points. </div> <p>As a result, the intrinsic parameters of the camera, its distortion coefficients, and the root mean square re-projection error (rmse) was computed:</p> <p>\begin{equation} \label{eq:intrinsics} K = \begin{array}{ccc} 2233 &amp; 0 &amp; 729 \ 0 &amp; 2233 &amp; 603 \ 0 &amp; 0 &amp; 1 \end{array} \end{equation}</p> <p>\begin{equation} \label{eq:distortion} distortion = \begin{pmatrix} 0 &amp; -5 &amp; 0 &amp; 0 &amp; 35 \end{pmatrix} \end{equation}</p> <p>\begin{equation} \label{eq:rmse} rmse = 1.815 \end{equation}</p> <p>At this point, the camera was considered known, as the root mean square re-projection error was within an admissible margin (i.e., approximately below 2 pixels).</p> <p>To localize the camera with respect to a known beacon, images of a said beacon (ID=4) were taken and the 2D corners of the beacon were detected (using <code class="language-plaintext highlighter-rouge">cv2.aruco.detectMarkers</code>, Figure 3) and matched to their 3D, real, pre-defined correspondences (mm_beacon_3D).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/robotics/eg-detection-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/robotics/eg-detection-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/robotics/eg-detection-1400.webp"></source> <img src="/assets/img/robotics/eg-detection.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. ArUco marker, with detected corners. </div> <p>Both 2D and 3D matches, along with the intrinsic parameters of the camera and its distortion were used to extract the extrinsic parameters of the camera (<code class="language-plaintext highlighter-rouge">localization.get_camera_position</code>): a perspective-n-point problem was solved with the given input to obtain the rotation and translation vectors (using <code class="language-plaintext highlighter-rouge">cv2.solvePnP</code>), the rotation vectors were transformed into a rotation matrix (using <code class="language-plaintext highlighter-rouge">cv2.Rodrigues</code>), and the center of the camera with respect to the beacon was computed:</p> <p>\begin{equation} \label{eq:center} center = - (KR)^{-1} (Kt) \end{equation}</p> <p>where K is the matrix of intrinsics, R is the rotation matrix and t is the traslation vector.</p> <p>Finally, to visualize the localization of the camera in 3D (Figure 4), the scene was set with an ArUco image setting the reference point of the scene (using a customed function, <code class="language-plaintext highlighter-rouge">localization.plot_scene</code>) and the real-time positions of the camera were plotted iteratively (using a custom function, <code class="language-plaintext highlighter-rouge">localization.plot_camera</code>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/robotics/eg-base-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/robotics/eg-base-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/robotics/eg-base-1400.webp"></source> <img src="/assets/img/robotics/eg-base.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/robotics/eg-localization-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/robotics/eg-localization-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/robotics/eg-localization-1400.webp"></source> <img src="/assets/img/robotics/eg-localization.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Representation of the scene with the ArUco marker (left) and the pose of the camera at different time instants (right). </div> <p>Localizing the camera with respect to the beacon is interesting in two ways. On the one hand, localizing the camera allows localizing the robot, since the camera tends to be in a known position to the robot. On the other hand, localizing the beacon can be extended to localizing a certain point in a known map (if the position of the beacon with respect to a map is known). Hence, geometric autolocalization using beacons exploits its potential when the robot is in a known position with respect to the camera, the camera is in a known position with respect to the beacon, and the beacon with respect to the map: <strong>the robot is located with respect to the map</strong>.</p> <p><em>Last edit: Mon 19, June 2023</em></p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Blanca Delgado. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: June 27, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>